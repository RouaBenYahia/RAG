{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP/FB6pBH/qbR30IXy+kN73",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RouaBenYahia/RAG/blob/all_types_with_mistral/Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "ivVwbEQKjjw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab6474d-8118-4f7a-a644-5a374e22a5e0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "TKeQ9tymn8cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef180eda-d4a2-4e75-aa78-a870bddec601"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/FineTuning-Rag"
      ],
      "metadata": {
        "id": "vNP251hAqkbj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "532409e2-6521-41b6-9a48-81015113601b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/FineTuning-Rag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "mcJRUwlUqmOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45f2e1cd-0536-47f0-8ba3-7eec6c47c4f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.ipynb  Finetuningp1.ipynb  htmltemplate.py\tStreamlit.ipynb\n",
            "app.py\t   Finetuningp2.ipynb  __pycache__\tStreamlit.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/FineTuning-Rag')\n"
      ],
      "metadata": {
        "id": "Wcy4H4IFn9AS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit langchain PyPDF2 faiss-cpu tiktoken huggingface-hub\n",
        "!pip install -U langchain_community langchain-huggingface\n",
        "!pip install python-docx"
      ],
      "metadata": {
        "id": "chNpz2xyU0hE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "244f6e89-44f7-420e-e6c6-8a3b581d2661"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.69)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.6)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.33.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (25.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (1.1.5)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key=userdata.get('huggingface')\n",
        "from huggingface_hub import login\n",
        "login(token=api_key)\n"
      ],
      "metadata": {
        "id": "IXJ1DWKbU2MB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "id": "Rj8OzFwbVBS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7adab21-1210-4183-cc48-c899ad64cf13"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K\n",
            "changed 22 packages in 980ms\n",
            "\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jOQbWy3UmuPK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rVWxIJR5UYvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51e19f99-2809-4006-8abe-e7a2d8a90e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Streamlit.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile Streamlit.py\n",
        "import streamlit as st\n",
        "from PyPDF2 import PdfReader\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "#from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from htmltemplate import css, bot_template, user_template\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pandas as pd\n",
        "import docx\n",
        "import io\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "\n",
        "def extract_text_from_files(uploaded_files):\n",
        "    all_text = \"\"\n",
        "\n",
        "    for uploaded_file in uploaded_files:\n",
        "        file_name = uploaded_file.name.lower()\n",
        "\n",
        "        # PDF\n",
        "        if file_name.endswith(\".pdf\"):\n",
        "            try:\n",
        "                pdf_reader = PdfReader(uploaded_file)\n",
        "                for page in pdf_reader.pages:\n",
        "                    text = page.extract_text()\n",
        "                    if text:\n",
        "                        all_text += text + \"\\n\"\n",
        "            except:\n",
        "                st.warning(f\"‚ùå Failed to read PDF: {uploaded_file.name}\")\n",
        "\n",
        "        # TXT\n",
        "        elif file_name.endswith(\".txt\"):\n",
        "            stringio = io.StringIO(uploaded_file.getvalue().decode(\"utf-8\"))\n",
        "            all_text += stringio.read() + \"\\n\"\n",
        "\n",
        "        # CSV\n",
        "        elif file_name.endswith(\".csv\"):\n",
        "            try:\n",
        "                df = pd.read_csv(uploaded_file)\n",
        "                all_text += df.to_string(index=False) + \"\\n\"\n",
        "            except:\n",
        "                st.warning(f\"‚ùå Failed to read CSV: {uploaded_file.name}\")\n",
        "\n",
        "        # DOCX\n",
        "        elif file_name.endswith(\".docx\"):\n",
        "            try:\n",
        "                doc = docx.Document(uploaded_file)\n",
        "                for para in doc.paragraphs:\n",
        "                    all_text += para.text + \"\\n\"\n",
        "            except:\n",
        "                st.warning(f\"‚ùå Failed to read DOCX: {uploaded_file.name}\")\n",
        "\n",
        "        # XLSX\n",
        "        elif file_name.endswith(\".xlsx\"):\n",
        "            try:\n",
        "                wb = load_workbook(uploaded_file, data_only=True)\n",
        "                for sheet in wb.sheetnames:\n",
        "                    ws = wb[sheet]\n",
        "                    for row in ws.iter_rows(values_only=True):\n",
        "                        row_text = \" | \".join([str(cell) if cell is not None else \"\" for cell in row])\n",
        "                        all_text += row_text + \"\\n\"\n",
        "            except:\n",
        "                st.warning(f\"‚ùå Failed to read XLSX: {uploaded_file.name}\")\n",
        "\n",
        "        else:\n",
        "            st.warning(f\"‚ö†Ô∏è Unsupported file type: {uploaded_file.name}\")\n",
        "\n",
        "    return all_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_text_chuncks(text):\n",
        "  text_splitter=CharacterTextSplitter(separator=\"\\n\",chunk_size=1000,chunk_overlap=300,length_function=len)\n",
        "  chunks=text_splitter.split_text(text)\n",
        "  return chunks\n",
        "\n",
        "\n",
        "\n",
        "def get_vectorstore(text_chunks):\n",
        "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={\"device\": \"cpu\"}\n",
        ")\n",
        "\n",
        "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    return vectorstore\n",
        "\n",
        " #tokenizer = AutoTokenizer.from_pretrained(\"rouabenyahia/FineTuningModel\")\n",
        "    #model = AutoModelForCausalLM.from_pretrained(\"rouabenyahia/FineTuningModel\")\n",
        "def get_conversation_chain(vectorstore):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.0,     # R√©ponses d√©terministes\n",
        "        do_sample=False\n",
        "    )\n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key='chat_history', return_messages=True\n",
        "    )\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "    Tu es un assistant expert. Utilise le contexte ci-dessous pour r√©pondre √† la question de l'utilisateur.\n",
        "    Si la r√©ponse ne se trouve pas dans les documents, dis simplement que tu ne sais pas.\n",
        "\n",
        "    Contexte:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    R√©ponse:\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
        "        #retriever=vectorstore.as_retriever(),\n",
        "        memory=memory,\n",
        "        combine_docs_chain_kwargs={\"prompt\": prompt}\n",
        "    )\n",
        "    return conversation_chain\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def handle_userinput(user_question):\n",
        "    retriever = st.session_state.conversation.retriever\n",
        "    docs = retriever.get_relevant_documents(user_question)\n",
        "    st.write(\"üîç Chunks retrouv√©s par le retriever:\", len(docs))\n",
        "\n",
        "    if len(docs) == 0:\n",
        "        st.warning(\"‚ö†Ô∏è Aucun passage du document n‚Äôa √©t√© trouv√© pour cette question. Essayez de reformuler.\")\n",
        "\n",
        "    response = st.session_state.conversation({'question': user_question})\n",
        "    st.session_state.chat_history = response['chat_history']\n",
        "\n",
        "    for i, message in enumerate(st.session_state.chat_history):\n",
        "        content = message.content\n",
        "        if i % 2 == 1:\n",
        "            if \"R√©ponse:\" in content:\n",
        "                content = content.split(\"R√©ponse:\")[-1].strip()\n",
        "            st.write(bot_template.replace(\"{{MSG}}\", content), unsafe_allow_html=True)\n",
        "        else:\n",
        "            st.write(user_template.replace(\"{{MSG}}\", content), unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Chat with your data\",\n",
        "    page_icon=\":books:\",\n",
        ")\n",
        "\n",
        "st.write(css,unsafe_allow_html=True)\n",
        "\n",
        "if \"conversation\" not in st.session_state:\n",
        "  st.session_state.conversation=None\n",
        "\n",
        "\n",
        "if \"chat_history\" not in st.session_state:\n",
        "  st.session_state.chat_history=None\n",
        "\n",
        "st.header(\"Chat with your data :books:\")\n",
        "\n",
        "#user_question=st.text_input(\"Ask a question about your documents\")\n",
        "#if user_question:\n",
        "  #handle_userinput(user_question)\n",
        "\n",
        "user_question = st.text_input(\"Ask a question about your documents\")\n",
        "\n",
        "if user_question and st.session_state.conversation is not None:\n",
        "    handle_userinput(user_question)\n",
        "elif user_question:\n",
        "    st.warning(\"üìÑ Veuillez d'abord uploader vos PDF et cliquer sur 'Process'.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with st.sidebar:\n",
        "  st.subheader(\"Your documents\")\n",
        "  uploaded_files = st.file_uploader(\n",
        "    \"Upload your files (.pdf, .txt, .csv, .docx, .xlsx) and click on 'Process'\",\n",
        "    type=[\"pdf\", \"txt\", \"csv\", \"docx\", \"xlsx\"],\n",
        "    accept_multiple_files=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "  if st.button(\"Process\"):\n",
        "    with st.spinner(\"Processing\"):\n",
        "\n",
        "      raw_text = extract_text_from_files(uploaded_files)\n",
        "      if not raw_text.strip():\n",
        "        st.error(\"‚ö†Ô∏è Aucun texte n‚Äôa √©t√© extrait. Les fichiers sont peut-√™tre vides, scann√©s ou illisibles.\")\n",
        "        st.stop()\n",
        "      else:\n",
        "        st.write(f\"üìÑ Nombre de caract√®res extraits: {len(raw_text)}\")\n",
        "\n",
        "\n",
        "\n",
        "      #st.write(raw_text)\n",
        "      text_chunks=get_text_chuncks(raw_text)\n",
        "      #st.write(\"‚úÖ Nombre de chunks extraits:\", len(text_chunks))\n",
        "\n",
        "      #st.write(text_chunks)\n",
        "      vectorstore = get_vectorstore(text_chunks)\n",
        "\n",
        "      #st.write(vectorstore)\n",
        "      #el conversation chain tawa\n",
        "      # on peut faire √ßa\n",
        "      st.session_state.conversation=get_conversation_chain(vectorstore)\n",
        "    st.success(\"‚úÖ The processing of the files is over, you can ask your questions now!\")\n",
        "    #st.session_state.conversation\n",
        "\n",
        "\n",
        "    #get the pdf text\n",
        "    #get the text chuncks\n",
        "    #create the vector store with the embeddings\n",
        "  #juste el api key mtaa hugging face bech nhothaa\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "7PWAYH22qQzr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9837d68b-1148-499d-e2f8-086872f283bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.ipynb  Finetuningp1.ipynb  htmltemplate.py\tStreamlit.ipynb\n",
            "app.py\t   Finetuningp2.ipynb  __pycache__\tStreamlit.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "! wget -q -O - ipv4.icanhazip.com\n",
        "\n"
      ],
      "metadata": {
        "id": "-a38ucOGc1qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "500b8fbb-0420-494f-8591-014005f3f926"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.63.37.141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "! streamlit run Streamlit.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "IMzRX_c6exHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f3e37da-77be-4225-8fb9-fc6a3fdaf9ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.63.37.141:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0Kyour url is: https://forty-eyes-thank.loca.lt\n"
          ]
        }
      ]
    }
  ]
}