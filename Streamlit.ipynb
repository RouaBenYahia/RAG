{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RouaBenYahia/RAG/blob/main/Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivVwbEQKjjw7",
        "outputId": "1a548329-966b-4f4c-d04c-237518b4fdb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKeQ9tymn8cd",
        "outputId": "072ffca5-4648-4a34-d402-d9e66158edb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNP251hAqkbj",
        "outputId": "c03e309c-3226-4ded-cac2-d515ee6aec19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/FineTuning-Rag\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/FineTuning-Rag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcJRUwlUqmOe",
        "outputId": "d8b3eb4d-a024-4e21-dba9-5e39fcc73f8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "app.ipynb  Finetuningp1.ipynb  htmltemplate.py\tStreamlit.ipynb\n",
            "app.py\t   Finetuningp2.ipynb  __pycache__\tStreamlit.py\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wcy4H4IFn9AS"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/FineTuning-Rag')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chNpz2xyU0hE",
        "outputId": "7f0410b7-07cd-410f-a32c-91a5ceb014c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m131.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.70)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.8)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.4 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.33.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.7.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (25.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.5)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-huggingface, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-huggingface-0.3.1 langchain_community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (25.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit langchain PyPDF2 faiss-cpu tiktoken huggingface-hub\n",
        "!pip install -U langchain_community langchain-huggingface\n",
        "!pip install python-docx\n",
        "!sudo apt-get install tesseract-ocr -y\n",
        "!pip install pytesseract pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXJ1DWKbU2MB"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key=userdata.get('huggingface')\n",
        "from huggingface_hub import login\n",
        "login(token=api_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rj8OzFwbVBS3",
        "outputId": "31ba5ece-207a-44aa-b3e7-f238a8bbc46f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ‹\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K\n",
            "added 22 packages in 2s\n",
            "\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "!npm install -g localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQbWy3UmuPK"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVWxIJR5UYvB",
        "outputId": "b701434f-b9f5-4a49-a1c8-aedda39aa1ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting Streamlit.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile Streamlit.py\n",
        "import streamlit as st\n",
        "from PyPDF2 import PdfReader\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "#from langchain_community.vectorstores import FAISS\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from htmltemplate import css, bot_template, user_template\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pandas as pd\n",
        "import docx\n",
        "import io\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "\n",
        "\n",
        "def extract_text_from_files(uploaded_files):\n",
        "    all_text = \"\"\n",
        "\n",
        "    for uploaded_file in uploaded_files:\n",
        "        file_name = uploaded_file.name.lower()\n",
        "\n",
        "        # PDF\n",
        "        if file_name.endswith(\".pdf\"):\n",
        "            try:\n",
        "                pdf_reader = PdfReader(uploaded_file)\n",
        "                for page in pdf_reader.pages:\n",
        "                    text = page.extract_text()\n",
        "                    if text:\n",
        "                        all_text += text + \"\\n\"\n",
        "            except:\n",
        "                st.warning(f\"âŒ Failed to read PDF: {uploaded_file.name}\")\n",
        "\n",
        "        # TXT\n",
        "        elif file_name.endswith(\".txt\"):\n",
        "            stringio = io.StringIO(uploaded_file.getvalue().decode(\"utf-8\"))\n",
        "            all_text += stringio.read() + \"\\n\"\n",
        "\n",
        "        # CSV\n",
        "        elif file_name.endswith(\".csv\"):\n",
        "            try:\n",
        "                df = pd.read_csv(uploaded_file)\n",
        "                all_text += df.to_string(index=False) + \"\\n\"\n",
        "            except:\n",
        "                st.warning(f\"âŒ Failed to read CSV: {uploaded_file.name}\")\n",
        "\n",
        "        # DOCX\n",
        "        elif file_name.endswith(\".docx\"):\n",
        "            try:\n",
        "                doc = docx.Document(uploaded_file)\n",
        "                for para in doc.paragraphs:\n",
        "                    all_text += para.text + \"\\n\"\n",
        "            except:\n",
        "                st.warning(f\"âŒ Failed to read DOCX: {uploaded_file.name}\")\n",
        "\n",
        "        # XLSX\n",
        "        elif file_name.endswith(\".xlsx\"):\n",
        "            try:\n",
        "                wb = load_workbook(uploaded_file, data_only=True)\n",
        "                for sheet in wb.sheetnames:\n",
        "                    ws = wb[sheet]\n",
        "                    for row in ws.iter_rows(values_only=True):\n",
        "                        row_text = \" | \".join([str(cell) if cell is not None else \"\" for cell in row])\n",
        "                        all_text += row_text + \"\\n\"\n",
        "            except:\n",
        "                st.warning(f\"âŒ Failed to read XLSX: {uploaded_file.name}\")\n",
        "\n",
        "        # PNG, JPG (image OCR)\n",
        "        elif file_name.endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "            try:\n",
        "                image = Image.open(uploaded_file)\n",
        "                text = pytesseract.image_to_string(image, lang='eng')  # ou 'fra' si câ€™est en franÃ§ais\n",
        "                all_text += text + \"\\n\"\n",
        "            except Exception as e:\n",
        "                st.warning(f\"âŒ Failed to extract text from image {uploaded_file.name}: {e}\")\n",
        "\n",
        "        else:\n",
        "            st.warning(f\"âš ï¸ Unsupported file type: {uploaded_file.name}\")\n",
        "\n",
        "    return all_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_text_chuncks(text):\n",
        "  text_splitter=CharacterTextSplitter(separator=\"\\n\",chunk_size=1000,chunk_overlap=300,length_function=len)\n",
        "  chunks=text_splitter.split_text(text)\n",
        "  return chunks\n",
        "\n",
        "\n",
        "\n",
        "def get_vectorstore(text_chunks):\n",
        "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={\"device\": \"cpu\"}\n",
        ")\n",
        "\n",
        "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "def get_conversation_chain(vectorstore):\n",
        "    #tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "    #model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"rouabenyahia/FineTuningModel\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"rouabenyahia/FineTuningModel\")\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.0,     # RÃ©ponses dÃ©terministes\n",
        "        do_sample=False\n",
        "    )\n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key='chat_history', return_messages=True\n",
        "    )\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "    Tu es un assistant expert. Utilise le contexte ci-dessous pour rÃ©pondre Ã  la question de l'utilisateur.\n",
        "    Si la rÃ©ponse ne se trouve pas dans les documents, dis simplement que tu ne sais pas.\n",
        "\n",
        "    Contexte:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    RÃ©ponse:\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
        "        #retriever=vectorstore.as_retriever(),\n",
        "        memory=memory,\n",
        "        combine_docs_chain_kwargs={\"prompt\": prompt}\n",
        "    )\n",
        "    return conversation_chain\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def handle_userinput(user_question):\n",
        "    retriever = st.session_state.conversation.retriever\n",
        "    docs = retriever.get_relevant_documents(user_question)\n",
        "    #st.write(\"ğŸ” Chunks retrouvÃ©s par le retriever:\", len(docs))\n",
        "\n",
        "    if len(docs) == 0:\n",
        "        st.warning(\"âš ï¸ Aucun passage du document nâ€™a Ã©tÃ© trouvÃ© pour cette question. Essayez de reformuler.\")\n",
        "\n",
        "    response = st.session_state.conversation({'question': user_question})\n",
        "    st.session_state.chat_history = response['chat_history']\n",
        "\n",
        "    for i, message in enumerate(st.session_state.chat_history):\n",
        "        content = message.content\n",
        "        if i % 2 == 1:\n",
        "            if \"RÃ©ponse:\" in content:\n",
        "                content = content.split(\"RÃ©ponse:\")[-1].strip()\n",
        "            st.write(bot_template.replace(\"{{MSG}}\", content), unsafe_allow_html=True)\n",
        "        else:\n",
        "            st.write(user_template.replace(\"{{MSG}}\", content), unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Chat with your data\",\n",
        "    page_icon=\":books:\",\n",
        ")\n",
        "\n",
        "st.write(css,unsafe_allow_html=True)\n",
        "\n",
        "if \"conversation\" not in st.session_state:\n",
        "  st.session_state.conversation=None\n",
        "\n",
        "\n",
        "if \"chat_history\" not in st.session_state:\n",
        "  st.session_state.chat_history=None\n",
        "\n",
        "st.header(\"Chat with your data :books:\")\n",
        "\n",
        "#user_question=st.text_input(\"Ask a question about your documents\")\n",
        "#if user_question:\n",
        "  #handle_userinput(user_question)\n",
        "\n",
        "user_question = st.text_input(\"Ask a question about your documents\")\n",
        "\n",
        "if user_question and st.session_state.conversation is not None:\n",
        "    handle_userinput(user_question)\n",
        "elif user_question:\n",
        "    st.warning(\"ğŸ“„ Veuillez d'abord uploader vos PDF et cliquer sur 'Process'.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with st.sidebar:\n",
        "  st.subheader(\"Your documents\")\n",
        "  uploaded_files = st.file_uploader(\n",
        "    \"Upload your files (.pdf, .txt, .csv, .docx, .xlsx, .png, .jpg) and click on 'Process'\",\n",
        "    type=[\"pdf\", \"txt\", \"csv\", \"docx\", \"xlsx\", \"png\", \"jpg\", \"jpeg\"],\n",
        "    accept_multiple_files=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  if st.button(\"Process\"):\n",
        "    with st.spinner(\"Processing\"):\n",
        "\n",
        "      raw_text = extract_text_from_files(uploaded_files)\n",
        "      if not raw_text.strip():\n",
        "        st.error(\"âš ï¸ Aucun texte nâ€™a Ã©tÃ© extrait. Les fichiers sont peut-Ãªtre vides, scannÃ©s ou illisibles.\")\n",
        "        st.stop()\n",
        "      else:\n",
        "        st.write(f\"ğŸ“„ Nombre de caractÃ¨res extraits: {len(raw_text)}\")\n",
        "\n",
        "\n",
        "\n",
        "      #st.write(raw_text)\n",
        "      text_chunks=get_text_chuncks(raw_text)\n",
        "      #st.write(\"âœ… Nombre de chunks extraits:\", len(text_chunks))\n",
        "\n",
        "      #st.write(text_chunks)\n",
        "      vectorstore = get_vectorstore(text_chunks)\n",
        "\n",
        "      #st.write(vectorstore)\n",
        "      #el conversation chain tawa\n",
        "      # on peut faire Ã§a\n",
        "      st.session_state.conversation=get_conversation_chain(vectorstore)\n",
        "    st.success(\"âœ… The processing of the files is over, you can ask your questions now!\")\n",
        "    #st.session_state.conversation\n",
        "\n",
        "\n",
        "    #get the pdf text\n",
        "    #get the text chuncks\n",
        "    #create the vector store with the embeddings\n",
        "  #juste el api key mtaa hugging face bech nhothaa\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PWAYH22qQzr",
        "outputId": "95f1ba96-a901-4a24-a59b-b63018c2edae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "app.ipynb  Finetuningp1.ipynb  htmltemplate.py\tStreamlit.ipynb\n",
            "app.py\t   Finetuningp2.ipynb  __pycache__\tStreamlit.py\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a38ucOGc1qq",
        "outputId": "0dc3ce83-57b6-43c6-ea6e-f4fba557e753"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35.237.252.3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "! wget -q -O - ipv4.icanhazip.com\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMzRX_c6exHf",
        "outputId": "ba9c60fe-1c2d-43d3-9825-f43283de4b0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ ‡\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ‹\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0K\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.236.152.101:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://few-drinks-trade.loca.lt\n",
            "2025-07-23 18:19:04.069611: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753294744.085452    5333 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753294744.090036    5333 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-23 18:19:04.105792: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import OpenAIEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing HuggingFaceInstructEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/usr/local/lib/python3.11/dist-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
            "\n",
            "`from langchain_community.chat_models import ChatOpenAI`.\n",
            "\n",
            "To install langchain-community run `pip install -U langchain-community`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
            "\n",
            "`from langchain_community.chat_models import ChatOpenAI`.\n",
            "\n",
            "To install langchain-community run `pip install -U langchain-community`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/langchain/llms/__init__.py:549: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
            "\n",
            "`from langchain_community.llms import HuggingFaceHub`.\n",
            "\n",
            "To install langchain-community run `pip install -U langchain-community`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/langchain/llms/__init__.py:549: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
            "\n",
            "`from langchain_community.llms import HuggingFaceHub`.\n",
            "\n",
            "To install langchain-community run `pip install -U langchain-community`.\n",
            "  warnings.warn(\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:14: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/usr/local/lib/python3.11/dist-packages/langchain/llms/__init__.py:549: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
            "\n",
            "`from langchain_community.llms import HuggingFacePipeline`.\n",
            "\n",
            "To install langchain-community run `pip install -U langchain-community`.\n",
            "  warnings.warn(\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import OpenAIEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing HuggingFaceInstructEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/usr/local/lib/python3.11/dist-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
            "\n",
            "`from langchain_community.chat_models import ChatOpenAI`.\n",
            "\n",
            "To install langchain-community run `pip install -U langchain-community`.\n",
            "  warnings.warn(\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:14: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import OpenAIEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing HuggingFaceInstructEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:14: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "modules.json: 100% 349/349 [00:00<00:00, 2.92MB/s]\n",
            "config_sentence_transformers.json: 100% 116/116 [00:00<00:00, 950kB/s]\n",
            "README.md: 10.5kB [00:00, 4.12MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 461kB/s]\n",
            "config.json: 100% 612/612 [00:00<00:00, 5.73MB/s]\n",
            "model.safetensors: 100% 90.9M/90.9M [00:01<00:00, 48.7MB/s]\n",
            "tokenizer_config.json: 100% 350/350 [00:00<00:00, 3.26MB/s]\n",
            "vocab.txt: 232kB [00:00, 1.55MB/s]\n",
            "tokenizer.json: 466kB [00:00, 3.01MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 965kB/s]\n",
            "config.json: 100% 190/190 [00:00<00:00, 1.67MB/s]\n",
            "tokenizer_config.json: 1.03kB [00:00, 4.92MB/s]\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 3.34MB/s]\n",
            "tokenizer.json: 3.51MB [00:00, 14.5MB/s]\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 4.09MB/s]\n",
            "config.json: 100% 613/613 [00:00<00:00, 6.82MB/s]\n",
            "adapter_config.json: 100% 875/875 [00:00<00:00, 7.84MB/s]\n",
            "config.json: 100% 571/571 [00:00<00:00, 5.01MB/s]\n",
            "model.safetensors.index.json: 100% 25.1k/25.1k [00:00<00:00, 39.4MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/4.54G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 0.00/9.94G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 203k/4.54G [00:02<12:53:15, 97.9kB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 43.1k/9.94G [00:02<144:48:36, 19.1kB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 15.9M/4.54G [00:02<10:04, 7.49MB/s]  \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 42.1M/9.94G [00:02<08:18, 19.9MB/s]    \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 126M/4.54G [00:03<01:12, 60.5MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 76.9M/9.94G [00:03<05:25, 30.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 277M/4.54G [00:03<00:31, 137MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 344M/4.54G [00:03<00:29, 143MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 183M/9.94G [00:04<02:40, 60.8MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 464M/4.54G [00:04<00:26, 156MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 485M/4.54G [00:04<00:28, 140MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 313M/9.94G [00:07<03:00, 53.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 553M/4.54G [00:07<00:59, 66.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 620M/4.54G [00:07<00:46, 84.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 687M/4.54G [00:07<00:37, 103MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 737M/4.54G [00:07<00:31, 123MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 384M/9.94G [00:07<02:40, 59.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 519M/9.94G [00:11<03:13, 48.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 804M/4.54G [00:11<01:21, 46.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   7% 720M/9.94G [00:11<01:39, 93.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 979M/4.54G [00:11<00:38, 92.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 1.05G/4.54G [00:11<00:31, 110MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 1.11G/4.54G [00:15<01:08, 50.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 854M/9.94G [00:15<02:32, 59.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 1.25G/4.54G [00:15<00:39, 82.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 921M/9.94G [00:15<02:06, 71.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  10% 983M/9.94G [00:15<01:47, 83.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  11% 1.05G/9.94G [00:15<01:26, 103MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 1.31G/4.54G [00:15<00:34, 92.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 1.36G/4.54G [00:16<00:35, 88.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  11% 1.12G/9.94G [00:17<01:47, 82.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 1.43G/4.54G [00:17<00:34, 90.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  12% 1.18G/9.94G [00:19<02:36, 56.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 1.50G/4.54G [00:19<00:51, 58.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 1.70G/4.54G [00:19<00:24, 116MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  13% 1.32G/9.94G [00:20<01:46, 80.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.77G/4.54G [00:20<00:26, 105MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  15% 1.45G/9.94G [00:20<01:20, 106MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.83G/4.54G [00:20<00:22, 123MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.92G/4.54G [00:21<00:17, 146MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.98G/4.54G [00:22<00:27, 92.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 2.05G/4.54G [00:23<00:28, 86.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  15% 1.49G/9.94G [00:23<02:31, 55.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 2.21G/4.54G [00:23<00:15, 153MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  16% 1.56G/9.94G [00:23<02:03, 68.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  16% 1.63G/9.94G [00:24<01:35, 86.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  18% 1.76G/9.94G [00:27<02:30, 54.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  19% 1.89G/9.94G [00:27<01:34, 85.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  20% 1.96G/9.94G [00:28<01:19, 101MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 2.28G/4.54G [00:28<00:45, 50.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  20% 2.02G/9.94G [00:28<01:17, 102MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  21% 2.09G/9.94G [00:29<01:10, 111MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 2.34G/4.54G [00:29<00:41, 53.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 2.41G/4.54G [00:29<00:32, 66.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 2.16G/9.94G [00:32<02:20, 55.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 2.41G/4.54G [00:40<00:32, 66.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 2.16G/9.94G [00:47<02:20, 55.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 2.23G/9.94G [00:48<10:07, 12.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  23% 2.29G/9.94G [00:49<07:41, 16.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 2.36G/9.94G [00:49<05:46, 21.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 2.48G/4.54G [00:51<03:18, 10.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 2.43G/9.94G [00:54<06:46, 18.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  25% 2.49G/9.94G [00:55<05:10, 24.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 2.54G/4.54G [00:56<03:01, 11.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 2.56G/9.94G [00:57<04:32, 27.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 2.63G/9.94G [01:04<07:09, 17.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 2.70G/9.94G [01:05<05:15, 23.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 2.76G/9.94G [01:05<03:44, 31.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.61G/4.54G [01:05<03:21, 9.58MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 2.83G/9.94G [01:05<02:53, 41.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 2.68G/4.54G [01:06<02:23, 13.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  29% 2.90G/9.94G [01:06<02:14, 52.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  30% 2.96G/9.94G [01:06<01:38, 71.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 2.75G/4.54G [01:06<01:42, 17.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  30% 3.03G/9.94G [01:06<01:17, 89.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 2.81G/4.54G [01:06<01:10, 24.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 2.88G/4.54G [01:07<00:49, 33.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 2.95G/4.54G [01:07<00:37, 42.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  31% 3.10G/9.94G [01:07<01:21, 84.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 3.01G/4.54G [01:08<00:31, 48.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  33% 3.23G/9.94G [01:08<01:07, 98.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  33% 3.30G/9.94G [01:08<00:54, 122MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  34% 3.37G/9.94G [01:09<00:44, 147MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 3.08G/4.54G [01:09<00:25, 56.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 3.15G/4.54G [01:09<00:18, 75.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 3.22G/4.54G [01:09<00:14, 92.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  35% 3.43G/9.94G [01:09<00:54, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  35% 3.50G/9.94G [01:10<00:43, 148MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  36% 3.57G/9.94G [01:10<00:35, 181MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 3.28G/4.54G [01:10<00:13, 96.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  37% 3.64G/9.94G [01:10<00:32, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  37% 3.70G/9.94G [01:10<00:28, 219MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  38% 3.77G/9.94G [01:10<00:23, 257MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 3.35G/4.54G [01:11<00:12, 96.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  39% 3.84G/9.94G [01:11<00:23, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  39% 3.90G/9.94G [01:11<00:21, 288MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 3.42G/4.54G [01:11<00:10, 106MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  40% 3.97G/9.94G [01:11<00:23, 249MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  41% 4.04G/9.94G [01:11<00:21, 274MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  41% 4.10G/9.94G [01:12<00:29, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 4.17G/9.94G [01:13<00:46, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  43% 4.24G/9.94G [01:13<00:44, 127MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 3.48G/4.54G [01:14<00:18, 57.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  44% 4.33G/9.94G [01:14<00:39, 142MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 3.55G/4.54G [01:14<00:15, 64.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 3.62G/4.54G [01:14<00:11, 83.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  44% 4.39G/9.94G [01:15<00:43, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  45% 4.46G/9.94G [01:15<00:34, 160MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 3.68G/4.54G [01:15<00:08, 98.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  46% 4.53G/9.94G [01:15<00:33, 161MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 3.80G/4.54G [01:15<00:05, 133MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 3.87G/4.54G [01:15<00:04, 165MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  46% 4.60G/9.94G [01:16<00:33, 162MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 3.94G/4.54G [01:16<00:03, 177MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  48% 4.73G/9.94G [01:16<00:25, 204MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  48% 4.80G/9.94G [01:16<00:21, 239MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 4.00G/4.54G [01:16<00:03, 167MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 4.07G/4.54G [01:16<00:02, 204MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 4.14G/4.54G [01:17<00:01, 247MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  49% 4.86G/9.94G [01:17<00:26, 190MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 4.21G/4.54G [01:17<00:01, 214MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 4.27G/4.54G [01:17<00:01, 256MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 4.93G/9.94G [01:17<00:29, 171MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 4.34G/4.54G [01:17<00:00, 294MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 5.00G/9.94G [01:17<00:23, 210MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 4.41G/4.54G [01:18<00:00, 261MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  51% 5.07G/9.94G [01:18<00:22, 215MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  52% 5.13G/9.94G [01:18<00:20, 231MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  52% 5.20G/9.94G [01:18<00:17, 270MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  53% 5.26G/9.94G [01:18<00:17, 271MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  54% 5.33G/9.94G [01:19<00:18, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  54% 5.40G/9.94G [01:19<00:24, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  55% 5.46G/9.94G [01:19<00:21, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  56% 5.53G/9.94G [01:19<00:16, 260MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  56% 5.60G/9.94G [01:20<00:15, 275MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  57% 5.67G/9.94G [01:20<00:14, 301MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  58% 5.73G/9.94G [01:20<00:19, 211MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 4.47G/4.54G [01:20<00:01, 64.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  59% 5.87G/9.94G [01:21<00:13, 294MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  60% 5.93G/9.94G [01:21<00:12, 317MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  60% 6.00G/9.94G [01:21<00:12, 321MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  61% 6.07G/9.94G [01:21<00:11, 333MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  62% 6.14G/9.94G [01:21<00:11, 335MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  63% 6.27G/9.94G [01:21<00:07, 473MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  64% 6.33G/9.94G [01:22<00:08, 449MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  64% 6.40G/9.94G [01:22<00:10, 340MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  65% 6.47G/9.94G [01:22<00:10, 332MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  66% 6.54G/9.94G [01:22<00:09, 349MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  66% 6.60G/9.94G [01:23<00:21, 155MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  67% 6.67G/9.94G [01:24<00:20, 160MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  68% 6.74G/9.94G [01:24<00:19, 165MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 4.54G/4.54G [01:25<00:00, 53.3MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00002.safetensors:  68% 6.80G/9.94G [01:25<00:21, 146MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  71% 7.07G/9.94G [01:25<00:08, 348MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  72% 7.21G/9.94G [01:25<00:06, 419MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  74% 7.34G/9.94G [01:26<00:07, 372MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  75% 7.47G/9.94G [01:26<00:06, 378MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  76% 7.54G/9.94G [01:31<00:36, 66.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  78% 7.74G/9.94G [01:31<00:19, 114MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  79% 7.83G/9.94G [01:31<00:15, 139MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  80% 7.96G/9.94G [01:31<00:11, 175MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  81% 8.03G/9.94G [01:32<00:11, 160MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  81% 8.10G/9.94G [01:32<00:12, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  82% 8.16G/9.94G [01:33<00:11, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  83% 8.23G/9.94G [01:33<00:09, 173MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  83% 8.30G/9.94G [01:33<00:07, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  84% 8.37G/9.94G [01:33<00:06, 225MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  85% 8.46G/9.94G [01:34<00:05, 273MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  86% 8.53G/9.94G [01:34<00:05, 261MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  86% 8.59G/9.94G [01:34<00:04, 279MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  87% 8.70G/9.94G [01:34<00:03, 348MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  89% 8.80G/9.94G [01:37<00:13, 86.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  89% 8.87G/9.94G [01:38<00:10, 100MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  90% 8.91G/9.94G [01:38<00:08, 116MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  90% 8.98G/9.94G [01:38<00:07, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  91% 9.05G/9.94G [01:38<00:05, 161MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  91% 9.08G/9.94G [01:38<00:04, 173MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  92% 9.14G/9.94G [01:38<00:03, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  94% 9.34G/9.94G [01:41<00:05, 107MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  96% 9.54G/9.94G [01:41<00:02, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  97% 9.61G/9.94G [01:41<00:01, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  97% 9.67G/9.94G [01:41<00:01, 227MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  98% 9.74G/9.94G [01:42<00:00, 249MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  99% 9.81G/9.94G [01:42<00:00, 268MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  99% 9.88G/9.94G [01:42<00:00, 294MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors: 100% 9.94G/9.94G [01:42<00:00, 96.9MB/s]\n",
            "Fetching 2 files: 100% 2/2 [01:43<00:00, 51.68s/it] \n",
            "Loading checkpoint shards: 100% 2/2 [00:31<00:00, 15.90s/it]\n",
            "generation_config.json: 100% 116/116 [00:00<00:00, 855kB/s]\n",
            "adapter_model.safetensors: 100% 369M/369M [00:15<00:00, 24.1MB/s]\n",
            "Device set to use cpu\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:130: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=pipe)\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:132: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import OpenAIEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing HuggingFaceInstructEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/usr/local/lib/python3.11/dist-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
            "\n",
            "`from langchain_community.chat_models import ChatOpenAI`.\n",
            "\n",
            "To install langchain-community run `pip install -U langchain-community`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/langchain/llms/__init__.py:549: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
            "\n",
            "`from langchain_community.llms import HuggingFaceHub`.\n",
            "\n",
            "To install langchain-community run `pip install -U langchain-community`.\n",
            "  warnings.warn(\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:14: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/usr/local/lib/python3.11/dist-packages/langchain/llms/__init__.py:549: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
            "\n",
            "`from langchain_community.llms import HuggingFacePipeline`.\n",
            "\n",
            "To install langchain-community run `pip install -U langchain-community`.\n",
            "  warnings.warn(\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:165: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(user_question)\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:171: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = st.session_state.conversation({'question': user_question})\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import OpenAIEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing HuggingFaceInstructEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/usr/local/lib/python3.11/dist-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
            "\n",
            "`from langchain_community.chat_models import ChatOpenAI`.\n",
            "\n",
            "To install langchain-community run `pip install -U langchain-community`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/langchain/llms/__init__.py:549: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
            "\n",
            "`from langchain_community.llms import HuggingFaceHub`.\n",
            "\n",
            "To install langchain-community run `pip install -U langchain-community`.\n",
            "  warnings.warn(\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:14: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/usr/local/lib/python3.11/dist-packages/langchain/llms/__init__.py:549: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
            "\n",
            "`from langchain_community.llms import HuggingFacePipeline`.\n",
            "\n",
            "To install langchain-community run `pip install -U langchain-community`.\n",
            "  warnings.warn(\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import OpenAIEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing HuggingFaceInstructEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:14: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import OpenAIEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing HuggingFaceInstructEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:14: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import OpenAIEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing HuggingFaceInstructEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:14: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import OpenAIEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing HuggingFaceInstructEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:14: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import OpenAIEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:5: LangChainDeprecationWarning: Importing HuggingFaceInstructEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:6: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "/content/drive/MyDrive/FineTuning-Rag/Streamlit.py:14: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
            "\n",
            ">> from langchain.vectorstores import FAISS\n",
            "\n",
            "with new imports of:\n",
            "\n",
            ">> from langchain_community.vectorstores import FAISS\n",
            "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
            "  from langchain.vectorstores import FAISS\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 23563.51it/s]\n",
            "Loading checkpoint shards:  50% 1/2 [00:45<00:45, 45.81s/it]"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "! streamlit run Streamlit.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMFLfEecyVniONSKeCLdxo7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}